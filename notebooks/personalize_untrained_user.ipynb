{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalizeで未学習のユーザに対するレコメンドの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from os import path\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dt = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "bucket_name = ''\n",
    "prefix = 'test-untrained-user'\n",
    "s3_prefix = f'personalize-work/{prefix}'\n",
    "user_ids_json_s3_path = path.join(s3_prefix, 'user_ids.jsonl')\n",
    "role_arn = 'arn:aws:iam::{account_id}:role/service-role/AmazonPersonalize-ExecutionRole'\n",
    "data_locations = {\n",
    "    'Users': path.join('s3://', bucket_name, s3_prefix, 'users.csv'),\n",
    "    'Items': path.join('s3://', bucket_name, s3_prefix, 'items.csv'),\n",
    "    'Interactions': path.join('s3://', bucket_name, s3_prefix, 'interactions.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize = boto3.Session().client('personalize')\n",
    "bucket = boto3.Session().resource('s3').Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('./ml-100k/u.user', sep='|', names=[\n",
    "    'USER_ID', 'AGE', 'GENDER', 'OCCUPATION', 'ZIP_CODE'\n",
    "], encoding='latin-1')\n",
    "users.set_index('USER_ID', inplace=True)\n",
    "users = users[['AGE', 'GENDER']]\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./ml-100k/u.item', sep='|', names=[\n",
    "    'ITEM_ID', 'TITLE', 'RELEASE_DATE', 'VIDEO_RELEASE_DATE', 'IMDB_URL', 'UNKNOWN', 'ACTION', 'ADVENTURE', 'ANIMATION', \"CHILDREN'S\", 'COMEDY', 'CRIME', 'DOCUMENTARY', 'DRAMA', 'FANTASY', 'FILM-NOIR', 'HORROR', 'MUSICAL', 'MYSTERY', 'ROMANCE', 'SCI-FI', 'THRILLER', 'WAR', 'WESTERN'\n",
    "], encoding='latin-1')\n",
    "items.set_index('ITEM_ID', inplace=True)\n",
    "def extract_genre(row):\n",
    "    return '|'.join([i for i, v in row[5:].items() if v == 1 ])\n",
    "items['GENRE'] = items.apply(extract_genre, axis=1)\n",
    "items = items[['TITLE', 'GENRE']]\n",
    "item_watch_count = df.groupby('ITEM_ID').size().sort_values(ascending=False)\n",
    "item_watch_count.name = 'watch_ct'\n",
    "items = items.join(item_watch_count)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 映画評価時のユーザの興味のあるジャンルをインタラクションデータに追加する(評価した映画のジャンルをランダムに抽出)\n",
    "df['GENRE_PREFERENCE'] = items.GENRE[df['ITEM_ID'].values].str.split('|').apply(lambda x: x[np.random.randint(len(x))]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データをアップロード\n",
    "users.to_csv(data_locations['Users'])\n",
    "items['GENRE'].to_csv(data_locations['Items']) # アイテムに関する情報はジャンルだけに絞る\n",
    "df.to_csv(data_locations['Interactions'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalizeのセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name=prefix\n",
    ")\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "\n",
    "max_time = time.time() + 3 * 60 * 60  # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn=dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response['datasetGroup']['status']\n",
    "    if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "        print(status)\n",
    "        break\n",
    "\n",
    "    time.sleep(60)\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィールドを定義する\n",
    "# 参考: https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html\n",
    "field_definitions = {\n",
    "    'Interactions': [\n",
    "        {\n",
    "            'name': 'USER_ID',\n",
    "            'type': 'string'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ITEM_ID',\n",
    "            'type': 'string'\n",
    "        },\n",
    "        {\n",
    "            'name': 'RATING',\n",
    "            'type': 'int'\n",
    "        },\n",
    "        {\n",
    "            'name': 'TIMESTAMP',\n",
    "            'type': 'long'\n",
    "        },\n",
    "        {\n",
    "            'name': 'GENRE_PREFERENCE',\n",
    "            'type': 'string',\n",
    "            'categorical': True\n",
    "        }\n",
    "    ],\n",
    "    'Users': [\n",
    "        {\n",
    "            'name': 'USER_ID',\n",
    "            'type': 'string'\n",
    "        },\n",
    "        {\n",
    "            'name': 'AGE',\n",
    "            'type': 'int'\n",
    "        },\n",
    "        {\n",
    "            'name': 'GENDER',\n",
    "            'type': 'string',\n",
    "            'categorical': True\n",
    "        }\n",
    "    ],\n",
    "    'Items': [\n",
    "        {\n",
    "            'name': 'ITEM_ID',\n",
    "            'type': 'string'\n",
    "        },\n",
    "        {\n",
    "            'name': 'GENRE',\n",
    "            'type': 'string',\n",
    "            'categorical': True\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = ['Interactions', 'Users', 'Items']\n",
    "dataset_import_job_arns = []\n",
    "for dataset_type in dataset_types:\n",
    "    # スキーマ作成\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name=f'{prefix}-{dataset_type}',\n",
    "        schema=json.dumps({\n",
    "            'type': 'record',\n",
    "            'name': dataset_type,\n",
    "            'namespace': 'com.amazonaws.personalize.schema',\n",
    "            'fields': field_definitions[dataset_type],\n",
    "            'version': '1.0'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # データセット作成\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name=f'{prefix}-{dataset_type}',\n",
    "        datasetType=dataset_type,\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        schemaArn=create_schema_response['schemaArn']\n",
    "    )\n",
    "\n",
    "    # データ読み込み\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName=f'{prefix}-{dataset_type}-{current_dt}',\n",
    "        datasetArn=create_dataset_response['datasetArn'],\n",
    "        dataSource={\n",
    "            'dataLocation': data_locations[dataset_type]\n",
    "        },\n",
    "        roleArn=role_arn\n",
    "    )\n",
    "    dataset_import_job_arns.append(create_dataset_import_job_response['datasetImportJobArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込みが完了するまで待機する\n",
    "def wait_for_importing_data(job_arn, max_wait_interval=3 * 60 * 60):\n",
    "    print(job_arn)\n",
    "    max_time = time.time() + max_wait_interval\n",
    "    while time.time() < max_time:\n",
    "        describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "            datasetImportJobArn=job_arn\n",
    "        )\n",
    "        status = describe_dataset_import_job_response['datasetImportJob']['status']\n",
    "\n",
    "        if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "            print(status)\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "        print('.', end='')\n",
    "\n",
    "for dataset_import_job_arn in dataset_import_job_arns:\n",
    "    wait_for_importing_data(dataset_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソリューションの作成\n",
    "# personalize.list_recipes()\n",
    "recipe_arns = [\n",
    "    'arn:aws:personalize:::recipe/aws-hrnn',\n",
    "    'arn:aws:personalize:::recipe/aws-hrnn-metadata',\n",
    "    'arn:aws:personalize:::recipe/aws-popularity-count'\n",
    "]\n",
    "\n",
    "solution_version_arns = []\n",
    "for recipe_arn in recipe_arns:\n",
    "    # ソリューションの作成\n",
    "    create_solution_response = personalize.create_solution(\n",
    "        name=f'{prefix}-{path.basename(recipe_arn)}',\n",
    "        datasetGroupArn=dataset_group_arn,\n",
    "        recipeArn=recipe_arn\n",
    "    )\n",
    "    solution_arn = create_solution_response['solutionArn']\n",
    "\n",
    "    # ソリューションバージョンの作成(モデルの学習)\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn=solution_arn\n",
    "    )\n",
    "    solution_version_arns.append(\n",
    "        create_solution_version_response['solutionVersionArn'])\n",
    "\n",
    "\n",
    "def wait_for_creating_solution_version(solution_version_arn, max_wait_interval=3 * 60 * 60):\n",
    "    print(solution_version_arn)\n",
    "    max_time = time.time() + max_wait_interval\n",
    "    while time.time() < max_time:\n",
    "        describe_solution_version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn=solution_version_arn\n",
    "        )\n",
    "        status = describe_solution_version_response['solutionVersion']['status']\n",
    "        if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "            print(status)\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "        print('.', end='')\n",
    "\n",
    "\n",
    "for solution_version_arn in solution_version_arns:\n",
    "    wait_for_creating_solution_version(solution_version_arn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for solution_version_arn in solution_version_arns:\n",
    "    response = personalize.get_solution_metrics(\n",
    "        solutionVersionArn=solution_version_arn\n",
    "    )\n",
    "    metrics[solution_version_arn.split('/')[-2]] = response['metrics']\n",
    "pd.DataFrame.from_dict(metrics, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レコメンド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_ids = list(users.index.values)\n",
    "\n",
    "# 未学習ユーザを追加\n",
    "target_user_ids.append(users.index.values.max() + 1)\n",
    "target_user_ids.append(users.index.values.max() + 2)\n",
    "user_ids = [json.dumps({'userId': str(user_id)}) for user_id in target_user_ids]\n",
    "bucket.Object(user_ids_json_s3_path).put(Body='\\n'.join(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_json_s3_uri = f's3://{bucket_name}/{user_ids_json_s3_path}'\n",
    "\n",
    "batch_job_arns = []\n",
    "for solution_version_arn in solution_version_arns:\n",
    "    solution_name = solution_version_arn.split('/')[-2]\n",
    "    solution_version = path.basename(solution_version_arn)\n",
    "    response = personalize.create_batch_inference_job(\n",
    "        jobName=f'{solution_version}',\n",
    "        solutionVersionArn=solution_version_arn,\n",
    "        numResults=100,\n",
    "        jobInput={\n",
    "            's3DataSource': {\n",
    "                'path': user_ids_json_s3_uri\n",
    "            }\n",
    "        },\n",
    "        jobOutput={\n",
    "            's3DataDestination': {\n",
    "                'path': path.join(path.dirname(user_ids_json_s3_uri), solution_name, solution_version, '')\n",
    "            }\n",
    "        },\n",
    "        roleArn=role_arn\n",
    "    )\n",
    "    batch_job_arns.append(response['batchInferenceJobArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_batch_inference_job(job_arn, max_wait_interval=3 * 60 * 60):\n",
    "    print(job_arn)\n",
    "    max_time = time.time() + max_wait_interval\n",
    "    while time.time() < max_time:\n",
    "        response = personalize.describe_batch_inference_job(\n",
    "            batchInferenceJobArn=job_arn\n",
    "        )\n",
    "        status = response['batchInferenceJob']['status']\n",
    "        if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "            print(status)\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "        print('.', end='')\n",
    "\n",
    "def transform_recommendation(dic):\n",
    "    return (\n",
    "        int(dic['input']['userId']), list(map(lambda x: int(x), dic['output']['recommendedItems']))\n",
    "    )\n",
    "        \n",
    "user_base_recommendations = {}\n",
    "recommends = {}\n",
    "for batch_job_arn in batch_job_arns:        \n",
    "    wait_for_batch_inference_job(batch_job_arn)\n",
    "    response = personalize.describe_batch_inference_job(batchInferenceJobArn=batch_job_arn)\n",
    "    job = response['batchInferenceJob']\n",
    "    file_s3_path = path.join(\n",
    "        *job['jobOutput']['s3DataDestination']['path'].split('/')[3:],\n",
    "        path.basename(job['jobInput']['s3DataSource']['path']) + '.out'\n",
    "    )\n",
    "\n",
    "    body = bucket.Object(file_s3_path).get()['Body'].read()\n",
    "    solution_name = job['solutionVersionArn'].split('/')[-2]\n",
    "    recommends[solution_name] = [transform_recommendation(json.loads(ss)) for ss in body.splitlines()]\n",
    "    user_base_recommendations[solution_name] = dict([transform_recommendation(json.loads(ss)) for ss in body.splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recommendation(user_id):\n",
    "    recoms = {}\n",
    "    for k, v in sorted(user_base_recommendations.items()):\n",
    "        recoms[k] = items[items.index.isin(v[user_id])].reset_index()\n",
    "    return pd.concat(recoms, axis=1)\n",
    "    \n",
    "    \n",
    "def fetch_interaction(user_id):\n",
    "    return df[df.USER_ID == user_id].join(items, on='ITEM_ID').sort_values('TIMESTAMP', ascending=False).set_index('ITEM_ID').loc[:, ['TITLE', 'GENRE', 'RATING', 'TIMESTAMP']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みユーザのインタラクションデータ\n",
    "fetch_interaction(1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みユーザのレコメンド内容\n",
    "fetch_recommendation(1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未学習ユーザのレコメンド内容\n",
    "fetch_recommendation(944)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未学習ユーザのレコメンド内容\n",
    "fetch_recommendation(945)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_version_arns2 = []\n",
    "response = personalize.list_solutions(datasetGroupArn=dataset_group_arn)\n",
    "for solution in response['solutions']:\n",
    "    solution_arn = solution['solutionArn']\n",
    "\n",
    "    # ソリューションバージョンの作成(モデルの学習)\n",
    "    create_solution_version_response = personalize.create_solution_version(\n",
    "        solutionArn=solution_arn\n",
    "    )\n",
    "    solution_version_arns2.append(\n",
    "        create_solution_version_response['solutionVersionArn'])\n",
    "\n",
    "\n",
    "def wait_for_creating_solution_version(solution_version_arn, max_wait_interval=3 * 60 * 60):\n",
    "    print(solution_version_arn)\n",
    "    max_time = time.time() + max_wait_interval\n",
    "    while time.time() < max_time:\n",
    "        describe_solution_version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn=solution_version_arn\n",
    "        )\n",
    "        status = describe_solution_version_response['solutionVersion']['status']\n",
    "        if status == 'ACTIVE' or status == 'CREATE FAILED':\n",
    "            print(status)\n",
    "            break\n",
    "\n",
    "        time.sleep(60)\n",
    "        print('.', end='')\n",
    "\n",
    "\n",
    "for solution_version_arn in solution_version_arns2:\n",
    "    wait_for_creating_solution_version(solution_version_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for solution_version_arn in solution_version_arns2:\n",
    "    response = personalize.get_solution_metrics(\n",
    "        solutionVersionArn=solution_version_arn\n",
    "    )\n",
    "    metrics[solution_version_arn.split('/')[-2]] = response['metrics']\n",
    "pd.DataFrame.from_dict(metrics, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_arns2 = []\n",
    "for solution_version_arn in solution_version_arns2:\n",
    "    solution_name = solution_version_arn.split('/')[-2]\n",
    "    solution_version = path.basename(solution_version_arn)\n",
    "    response = personalize.create_batch_inference_job(\n",
    "        jobName=f'{solution_version}',\n",
    "        solutionVersionArn=solution_version_arn,\n",
    "        numResults=100,\n",
    "        jobInput={\n",
    "            's3DataSource': {\n",
    "                'path': user_ids_json_s3_uri\n",
    "            }\n",
    "        },\n",
    "        jobOutput={\n",
    "            's3DataDestination': {\n",
    "                'path': path.join(path.dirname(user_ids_json_s3_uri), solution_name, solution_version, '')\n",
    "            }\n",
    "        },\n",
    "        roleArn=role_arn\n",
    "    )\n",
    "    batch_job_arns2.append(response['batchInferenceJobArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_base_recommendations2 = {}\n",
    "recommends2 = {}\n",
    "for batch_job_arn in batch_job_arns2:        \n",
    "    wait_for_batch_inference_job(batch_job_arn)\n",
    "    response = personalize.describe_batch_inference_job(batchInferenceJobArn=batch_job_arn)\n",
    "    job = response['batchInferenceJob']\n",
    "    file_s3_path = path.join(\n",
    "        *job['jobOutput']['s3DataDestination']['path'].split('/')[3:],\n",
    "        path.basename(job['jobInput']['s3DataSource']['path']) + '.out'\n",
    "    )\n",
    "\n",
    "    body = bucket.Object(file_s3_path).get()['Body'].read()\n",
    "    solution_name = job['solutionVersionArn'].split('/')[-2]\n",
    "    recommends2[solution_name] = [transform_recommendation(json.loads(ss)) for ss in body.splitlines()]\n",
    "    user_base_recommendations2[solution_name] = dict([transform_recommendation(json.loads(ss)) for ss in body.splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recommendation2(user_id):\n",
    "    recoms = {}\n",
    "    for k, v in sorted(user_base_recommendations2.items()):\n",
    "        recoms[k] = items[items.index.isin(v[user_id])].reset_index()\n",
    "    return pd.concat(recoms, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_recommendation2(1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_recommendation2(944)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
